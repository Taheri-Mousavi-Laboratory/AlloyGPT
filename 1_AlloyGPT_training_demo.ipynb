{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c517db6-b916-4da3-91e7-1d223efd274f",
   "metadata": {},
   "source": [
    "# Training demo for AlloyGPT\n",
    "\n",
    "## Notes:\n",
    "> - [ ] This demo is to show how to train AlloyGPT with a formated language dataset based Thermo-Calc dataset\n",
    "> - [ ] Due to license restriction of Thermo-Calc, you will be asked for \"dataset_key\" to access the dataset for training.\n",
    "\n",
    "## Ref:\n",
    "> - 1. \n",
    "\n",
    "#### Bo Ni, Feb 18, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d1d186-ede4-4629-a3e3-1fd45a03e5cd",
   "metadata": {},
   "source": [
    "## 1. Check the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6b006b1-086e-4dc5-8ff1-5f76f18310ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is : \n",
      " /trace/group/tmousavi/bni2/1_JupyterGit/workspace/AlloyGPT_InternalTest_0\n",
      "\n",
      "What we get in hardware: \n",
      " Fri Feb 21 16:24:06 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     On  | 00000000:25:00.0 Off |                    0 |\n",
      "|  0%   30C    P0              69W / 300W |   8214MiB / 46068MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   1096576      C   ...ernalTest_0/AlloyGPT_env/bin/python     8202MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "\n",
      "The VirEnv kernal in action:  AlloyGPT_env\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trace/group/tmousavi/bni2/1_JupyterGit/workspace/AlloyGPT_InternalTest_0/AlloyGPT_env/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we have in software: \n",
      " Torch version: 2.6.0+cu124\n",
      "Python:  3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]\n",
      "What hardware the software see:\n",
      "cuda:0\n",
      "# of GPU 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, sys\n",
    "print('Here is : \\n', os.popen('pwd').read())\n",
    "print('What we get in hardware: \\n', os.popen('nvidia-smi').read())\n",
    "kernel_name = os.path.basename(sys.executable.replace(\"/bin/python\",\"\"))\n",
    "print (\"The VirEnv kernal in action: \", kernel_name)\n",
    "#\n",
    "# # make it work for both py and notebook\n",
    "# try:\n",
    "#     from jupyter_client import kernelspec\n",
    "#     spec = kernelspec.get_kernel_spec(kernel_name)\n",
    "#     print(\"Path to it: \", spec.resource_dir)\n",
    "# except:\n",
    "#     print (\"This suppose to be a .py run\")\n",
    "# # /path/to/my/kernel\n",
    "import torch\n",
    "print(\"What we have in software: \\n Torch version:\", torch.__version__)\n",
    "print('Python: ', sys.version) # no switch case code\n",
    "#\n",
    "print('What hardware the software see:')\n",
    "device = torch.device(\n",
    "    \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(device)\n",
    "num_of_gpus = torch.cuda.device_count()\n",
    "print(\"# of GPU\", num_of_gpus)\n",
    "#\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd4277-9d36-4c52-b76c-9e8f3fbe0092",
   "metadata": {},
   "source": [
    "## 2. Configurate the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5231c2dd-5e6f-43ee-996e-7a5979383ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debug\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d78269ba-9c76-4806-80dd-70d3900f23ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'AlloyGPT.TestPack' from '/trace/group/tmousavi/bni2/1_JupyterGit/workspace/AlloyGPT_InternalTest_0/AlloyGPT/TestPack.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in source code packages\n",
    "\n",
    "import AlloyGPT.UtilityPack as UtilityPack\n",
    "importlib.reload(UtilityPack)\n",
    "#\n",
    "import AlloyGPT.DataPack as DataPack\n",
    "importlib.reload(DataPack)\n",
    "#\n",
    "import AlloyGPT.ModelPack as ModelPack\n",
    "importlib.reload(ModelPack)\n",
    "#\n",
    "import AlloyGPT.TrainPack as TrainPack\n",
    "importlib.reload(TrainPack)\n",
    "#\n",
    "import AlloyGPT.TestPack as TestPack\n",
    "importlib.reload(TestPack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d865c030-81ec-499d-a1ab-2f9cdc1ce297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in additional packages\n",
    "\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import pandas as pd\n",
    "# ++\n",
    "import numpy as np\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80ada5f8-b70e-4664-9b76-66e832e58132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# code_dir = '/content/AlloyGPT_InternalTest_0/' # for colab run\n",
    "# \n",
    "code_dir = './' # for local run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "229354e2-f53f-4fe2-b517-a1c7effc1f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your key to the dataset: \n",
      "\n",
      " ········\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset_key = getpass.getpass(prompt=\"Please enter your key to the dataset: \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4df615d8-53aa-4cfe-af8b-b0383083a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control key\n",
    "\n",
    "# ===============================================\n",
    "# Global control key setup: may into yaml file\n",
    "# ===============================================\n",
    "# control key to be shared everywhere\n",
    "#\n",
    "CKeys = {}\n",
    "CKeys['Working_Mode'] = 1 # 1 # 0 # 1 # 2\n",
    "# 0: prepare dataset: run only on single node\n",
    "# 1: training,\n",
    "# 2: testing\n",
    "\n",
    "# only for restart of training part\n",
    "CKeys['IF_FirstRun'] = 1 # 1 # 2\n",
    "CKeys['Resume_from_where'] = 'LAST'\n",
    "# 1: train for 1st run; 2: other training loop\n",
    "\n",
    "CKeys['Problem_ID'] = 6\n",
    "# 1: Transformer-based causal LM: GPT2\n",
    "# 2: Mamba-based causal LM\n",
    "# 3: T-based causal LM + HF dataset on AlloyLan\n",
    "# 4: selfmade S6\n",
    "# 5: BPE tokenizer + hf-S6\n",
    "# 6: BPE tokenizer + GPT2\n",
    "\n",
    "# where to pick up the checkpoint\n",
    "CKeys['Resume_from_where'] = \"LAST\" # \"BEST\"\n",
    "\n",
    "# On Debug\n",
    "# CKeys['Debug']=1\n",
    "CKeys['Debug']=0\n",
    "\n",
    "if CKeys['Debug'] == 1:\n",
    "    CKeys['Debug_Data'] = 1\n",
    "    CKeys['Debug_Model'] = 1\n",
    "    CKeys['Debug_Train'] = 1\n",
    "    CKeys['Debug_Test'] = 1\n",
    "    # for On cluster run\n",
    "    CKeys['if_slient_run']=0\n",
    "else:\n",
    "    CKeys['Debug_Data']=0\n",
    "    CKeys['Debug_Model']=0\n",
    "    CKeys['Debug_Train']=0\n",
    "    CKeys['Debug_Test']=0\n",
    "    # for On cluster run\n",
    "    CKeys['if_slient_run']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8d703a8-4404-41d9-9bd9-f346a5367260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check CKeys: \n",
      "\n",
      "Working_Mode: 1\n",
      "IF_FirstRun: 1\n",
      "Resume_from_where: LAST\n",
      "Problem_ID: 6\n",
      "Debug: 0\n",
      "Debug_Data: 0\n",
      "Debug_Model: 0\n",
      "Debug_Train: 0\n",
      "Debug_Test: 0\n",
      "if_slient_run: 1\n"
     ]
    }
   ],
   "source": [
    "print (\"Check CKeys: \\n\")\n",
    "for this_key in CKeys.keys():\n",
    "    print (f\"{this_key}: {CKeys[this_key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd160dd5-a6d0-4b91-9d9d-dc825f5f1895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem Review: \n",
      "Problem type:  6\n",
      "Debug mode:  0\n",
      "Working mode:  1\n"
     ]
    }
   ],
   "source": [
    "# problem review:\n",
    "print (\"Problem Review: \")\n",
    "print('Problem type: ', CKeys['Problem_ID'])\n",
    "print('Debug mode: ', CKeys['Debug'])\n",
    "print('Working mode: ', CKeys['Working_Mode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b9655df-6056-4148-a6c0-29e28b1d785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# Parameter Keys: may think of moving this into\n",
    "# a yaml file\n",
    "# 1. set for the first time\n",
    "# 2. reload in sequential runs\n",
    "# ===============================================\n",
    "#\n",
    "PKeys = {}\n",
    "PKeys['prefix']='Causal_LM_TwoWays'\n",
    "\n",
    "PKeys['wk_path']='./training_resu/'\n",
    "#\n",
    "# may add keys for lower level\n",
    "# ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
    "# on data + tokenizer\n",
    "Data_PKeys = {}\n",
    "Data_PKeys['data_dir']=PKeys['wk_path']+'/0_dataprocess/'\n",
    "Data_PKeys['tokenizer_dir']=Data_PKeys['data_dir']+'/tokenizer'\n",
    "# problem-specified ones\n",
    "Data_PKeys['tokenizer_type']='BPE_customerized_0' # 'Byte_Tokenizer'\n",
    "# Data_PKeys['tokenizer_file']='/trace/group/tmousavi/bni2/1_JupyterGit/workspace/Test_NanoGPT_1/Local_Store/cBPE_tokenizer_ver1.json'\n",
    "Data_PKeys['tokenizer_file']=code_dir+'/assets/cBPE_tokenizer_ver1.json'\n",
    "Data_PKeys['train_ratio']=0.900\n",
    "Data_PKeys['valid_ratio']=0.001\n",
    "Data_PKeys['fix_random']=12345\n",
    "Data_PKeys['context_length']=1024 # length of text window\n",
    "# not working for google colab T4\n",
    "Data_PKeys['batch_size'] = 12 # 12 # 8 # 4 # 8 # 16 # 24 # 32\n",
    "#\n",
    "Data_PKeys['batch_size'] = 4 # 12 # 8 # 4 # 8 # 16 # 24 # 32\n",
    "# use hf-dataset:\n",
    "Data_PKeys['hf_data_repo'] = 'Bo-Ni/Al_alloy_CALPHAD_test_4_full' # 'Bo-Ni/Al_alloy_CALPHAD_test_3_full'\n",
    "#\n",
    "# ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
    "# on model\n",
    "Model_PKeys = {}\n",
    "Model_PKeys['model_dir']=PKeys['wk_path']+'/1_model/'\n",
    "Model_PKeys['model_type'] = 6 # 5: hfS6; 6: GPT2\n",
    "Model_PKeys['model_args'] = dict(\n",
    "    vocab_size=256, # len of the dict of the tokenizer\n",
    "    n_layer = 36, # 24 # 6 # num of MHA blocks\n",
    "    block_size = 1024,\n",
    "    n_embd = 1024,\n",
    "    # inside one MHA layer\n",
    "    n_head = 16,\n",
    "    bias = False,\n",
    "    dropout = 0.2,\n",
    ")\n",
    "#\n",
    "# ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
    "# on training\n",
    "Train_PKeys = {}\n",
    "Train_PKeys['out_dir'] = Model_PKeys['model_dir'] + \"/training_dir/\"\n",
    "# + customize ones +\n",
    "Train_PKeys['batch_size'] = Data_PKeys['batch_size']\n",
    "Train_PKeys['block_size'] = Data_PKeys['context_length']\n",
    "Train_PKeys['vocab_size'] = Model_PKeys['model_args']['vocab_size']\n",
    "Train_PKeys['num_train_epochs'] = 1 if CKeys['Debug']==1 else 5 # 2\n",
    "Train_PKeys['gradient_accumulation_steps'] = 8 # 40 # 1 # this's GAS\n",
    "#\n",
    "# AdamW optimizer, may put this into model class\n",
    "Train_PKeys['weight_decay'] = 1e-1\n",
    "Train_PKeys['beta1'] = 0.9\n",
    "Train_PKeys['beta2'] = 0.95\n",
    "# 0.99  # make a bit bigger because number of tokens per iter is small\n",
    "Train_PKeys['grad_clip'] = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "#\n",
    "# Learning Plan:\n",
    "# (linear warmup) + (cosine schedule) for lr_decay_iters + constant at min_lr\n",
    "# Train_PKeys['max_iters'] = math.ceil(\n",
    "#     len(train_dataloader)/Train_PKeys['gradient_accumulation_steps']\n",
    "# ) * Train_PKeys['num_train_epochs'] # in GAS\n",
    "# Train_PKeys['lr_decay_iters'] = Train_PKeys['max_iters']\n",
    "Train_PKeys['warmup_iters'] = 1_000 # for large one # in GAS\n",
    "Train_PKeys['learning_rate'] = 6e-4\n",
    "Train_PKeys['decay_lr'] = True\n",
    "# whether to decay the learning rate\n",
    "# True: linear warmup + cosine decay + constant\n",
    "# False: constant\n",
    "#\n",
    "Train_PKeys['min_lr'] = Train_PKeys['learning_rate']*1.e-1\n",
    "#\n",
    "# reporting and recording\n",
    "Train_PKeys['report_1_trai_loss_this_GAS'] = 2 if CKeys['Debug']==1 else 20\n",
    "Train_PKeys['report_2_vali_pred_this_GAS'] = 2 if CKeys['Debug']==1 else 200\n",
    "Train_PKeys['report_3_save_mode_this_GAS'] = \\\n",
    "    Train_PKeys['report_2_vali_pred_this_GAS']*2 \\\n",
    "    if CKeys['Debug']==1 else \\\n",
    "    Train_PKeys['report_2_vali_pred_this_GAS']*2\n",
    "#\n",
    "# OTHERS: parallel setup\n",
    "# DDP settings\n",
    "Train_PKeys['backend'] = 'nccl' # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "Train_PKeys['device'] = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "Train_PKeys['dtype'] = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "Train_PKeys['compile'] = True # use PyTorch 2.0 to compile the model to be faster\n",
    "#\n",
    "\n",
    "# ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
    "# on testing\n",
    "Test_PKeys = {}\n",
    "Test_PKeys['out_dir'] = Model_PKeys['model_dir'] + \"/test_dir/\"\n",
    "#\n",
    "# TBA: useful keys for testing\n",
    "# do a initialization\n",
    "Test_PKeys['num_samples'] = 2 # 10\n",
    "# number of samples to draw\n",
    "Test_PKeys['max_new_tokens'] = 1024 # 500\n",
    "# number of tokens generated in each sample\n",
    "Test_PKeys['temperature'] = 0.8\n",
    "# 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "Test_PKeys['top_k'] = 200\n",
    "# retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "Test_PKeys['seed'] = 1337\n",
    "Test_PKeys['device'] = Train_PKeys['device']\n",
    "# examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "Test_PKeys['dtype'] = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "# 'float32' or 'bfloat16' or 'float16'\n",
    "Test_PKeys['compile'] = Train_PKeys['compile']\n",
    "\n",
    "# pack keys for later\n",
    "PKeys['Data_PKeys'] = Data_PKeys\n",
    "PKeys['Model_PKeys'] = Model_PKeys\n",
    "PKeys['Train_PKeys'] = Train_PKeys\n",
    "PKeys['Test_PKeys'] = Test_PKeys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42ed0521-6c21-46a3-aac6-f3086286dda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "setup folder structures and parameter files\n",
      "===============================================\n",
      "Working path exists or not:  True\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# setup folder structures and parameter files\n",
    "# place_holder\n",
    "# ===============================================\n",
    "print (\"===============================================\")\n",
    "print (\"setup folder structures and parameter files\")\n",
    "print (\"===============================================\")\n",
    "#\n",
    "# 1. global dir\n",
    "print ('Working path exists or not: ', os.path.exists(PKeys['wk_path']))\n",
    "if not os.path.exists(PKeys['wk_path']):\n",
    "    UtilityPack.create_path(PKeys['wk_path'])\n",
    "\n",
    "# 2. some global keys for recalling\n",
    "# store info for later\n",
    "PKeys['pk_data_pack']=PKeys['wk_path']+'/data_pack.pickle'\n",
    "PKeys['pk_model_pack']=PKeys['wk_path']+'/model_pack.pickle'\n",
    "PKeys['pk_train_pack']=PKeys['wk_path']+'/train_pack.pickle'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee3117df-16c8-4aa6-b9be-7a86e220e2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working path:  ./training_resu/\n"
     ]
    }
   ],
   "source": [
    "print (f\"Working path: \", PKeys['wk_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a8a3e03-e48a-4bac-8339-e4805accd435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Clean EVERYTHING in the dir if 1st...\n",
      "===============================================\n",
      "clean the slade...\n",
      "excute rm -r ./training_resu/\n",
      "Creating the given path...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# clean EVERYTHING in the dir if 1st\n",
    "#\n",
    "# if CKeys['Working_Mode']==1 and CKeys['IF_FirstRun']==1:\n",
    "print (\"===============================================\")\n",
    "print (\"Clean EVERYTHING in the dir if 1st...\")\n",
    "print (\"===============================================\")\n",
    "#\n",
    "if CKeys['Working_Mode']==1 and CKeys['IF_FirstRun']==1:\n",
    "    #\n",
    "    if os.path.exists(PKeys['wk_path']):\n",
    "        cmd_line=f\"rm -r {PKeys['wk_path']}\"\n",
    "        print(\"clean the slade...\")\n",
    "        print(f\"excute {cmd_line}\")\n",
    "        os.popen(cmd_line).read()\n",
    "        #\n",
    "    # create dir for working space\n",
    "    UtilityPack.create_path(PKeys['wk_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e31e43-98c8-4c65-810a-377bdd14b2e5",
   "metadata": {},
   "source": [
    "## 3. On dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d8c0814-9ca5-48fc-a9b1-eeaa12a2d5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in\n",
    "# use HF infurstructures\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    load_from_disk,\n",
    "    load_dataset_builder,\n",
    "    get_dataset_split_names,\n",
    "    DatasetDict,\n",
    "    load_from_disk,\n",
    ")\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bfd18a8-bcf4-4b12-ba83-ac063b6bc8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.2\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3746a513-758a-4f2b-9a53-c90e13aa78e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "setup the dataset key\n",
      "===================================================\n",
      "Get the following keys...\n",
      "data_dir: \n",
      "./training_resu//0_dataprocess/\n",
      "tokenizer_dir: \n",
      "./training_resu//0_dataprocess//tokenizer\n",
      "tokenizer_type: \n",
      "BPE_customerized_0\n",
      "tokenizer_file: \n",
      ".//assets/cBPE_tokenizer_ver1.json\n",
      "train_ratio: \n",
      "0.9\n",
      "valid_ratio: \n",
      "0.001\n",
      "fix_random: \n",
      "12345\n",
      "context_length: \n",
      "1024\n",
      "batch_size: \n",
      "4\n",
      "hf_data_repo: \n",
      "Bo-Ni/Al_alloy_CALPHAD_test_4_full\n",
      "Create folders if needed...\n",
      "Creating the given path...\n",
      "Done.\n",
      "Creating the given path...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================\n",
    "# setup the dataset key\n",
    "# ===================================================\n",
    "#\n",
    "if CKeys['Working_Mode']==1 and CKeys['IF_FirstRun']==1:\n",
    "    print (\"===================================================\")\n",
    "    print (\"setup the dataset key\")\n",
    "    print (\"===================================================\")\n",
    "\n",
    "    # first initialize the key\n",
    "    DataKeys={}\n",
    "    # pass on the keys\n",
    "    print (\"Get the following keys...\")\n",
    "    for this_key in PKeys['Data_PKeys'].keys():\n",
    "        print (\"{}: \\n{}\".format(this_key, PKeys['Data_PKeys'][this_key]))\n",
    "        DataKeys[this_key] = PKeys['Data_PKeys'][this_key]\n",
    "\n",
    "    print (\"Create folders if needed...\")\n",
    "    # 1. creat subdir for datapart\n",
    "    UtilityPack.create_path(DataKeys['data_dir'])\n",
    "    # tokenizer folder\n",
    "    UtilityPack.create_path(DataKeys['tokenizer_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e55f17b5-6999-4e1f-ac12-d823ad3e91a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "process the dataset\n",
      "===================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since Bo-Ni/Al_alloy_CALPHAD_test_4_full couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /trace/home/bni2/.cache/huggingface/datasets/Bo-Ni___al_alloy_calphad_test_4_full/default/0.0.0/96b80fc0f9e668d7916a6bc2208fa0130858e801 (last modified on Tue Feb 18 15:19:15 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw dataset:\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['AlMolePC', 'NiMolePC', 'ErMolePC', 'ZrMolePC', 'YMolePC', 'YbMolePC', 'L12MolePC', 'TerneryMolePC', 'Al3NiMolePC', 'Al3ZrMolePC', 'ScheilL12MolePC', 'ScheilTernaryMolePC', 'ScheilAl3NiMolePC', 'ScheilAl3ZrMolePC', 'NZ_BulkResistivity', 'NZ_Misfit', 'NZ_CoarseningMetric', 'NZ_Freezing_Range_From_fccAl', 'CSC', 'NZ_HCS'],\n",
      "        num_rows: 523599\n",
      "    })\n",
      "})\n",
      "seperate raw dataset:\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['AlMolePC', 'NiMolePC', 'ErMolePC', 'ZrMolePC', 'YMolePC', 'YbMolePC', 'L12MolePC', 'TerneryMolePC', 'Al3NiMolePC', 'Al3ZrMolePC', 'ScheilL12MolePC', 'ScheilTernaryMolePC', 'ScheilAl3NiMolePC', 'ScheilAl3ZrMolePC', 'NZ_BulkResistivity', 'NZ_Misfit', 'NZ_CoarseningMetric', 'NZ_Freezing_Range_From_fccAl', 'CSC', 'NZ_HCS'],\n",
      "        num_rows: 471239\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['AlMolePC', 'NiMolePC', 'ErMolePC', 'ZrMolePC', 'YMolePC', 'YbMolePC', 'L12MolePC', 'TerneryMolePC', 'Al3NiMolePC', 'Al3ZrMolePC', 'ScheilL12MolePC', 'ScheilTernaryMolePC', 'ScheilAl3NiMolePC', 'ScheilAl3ZrMolePC', 'NZ_BulkResistivity', 'NZ_Misfit', 'NZ_CoarseningMetric', 'NZ_Freezing_Range_From_fccAl', 'CSC', 'NZ_HCS'],\n",
      "        num_rows: 523\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['AlMolePC', 'NiMolePC', 'ErMolePC', 'ZrMolePC', 'YMolePC', 'YbMolePC', 'L12MolePC', 'TerneryMolePC', 'Al3NiMolePC', 'Al3ZrMolePC', 'ScheilL12MolePC', 'ScheilTernaryMolePC', 'ScheilAl3NiMolePC', 'ScheilAl3ZrMolePC', 'NZ_BulkResistivity', 'NZ_Misfit', 'NZ_CoarseningMetric', 'NZ_Freezing_Range_From_fccAl', 'CSC', 'NZ_HCS'],\n",
      "        num_rows: 51837\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ===================================================\n",
    "# process the dataset\n",
    "# ===================================================\n",
    "#\n",
    "if CKeys['Working_Mode']==1 and CKeys['IF_FirstRun']==1:\n",
    "    print (\"===================================================\")\n",
    "    print (\"process the dataset\")\n",
    "    print (\"===================================================\")\n",
    "    # process\n",
    "    # 1. download the dict from HF\n",
    "    dataset_dict = load_dataset(\n",
    "        DataKeys['hf_data_repo'],\n",
    "        token=dataset_key,\n",
    "        revision=\"main\",\n",
    "        \n",
    "    )\n",
    "    # # 1.5 alternative way:\n",
    "    # # from datasets import load_from_disk\n",
    "    # dataset_dict = load_from_disk(data_dict_path)\n",
    "\n",
    "    print (\"raw dataset:\\n\")\n",
    "    print (dataset_dict)\n",
    "    #\n",
    "    # 2.seperate into three\n",
    "    dataset_dict_sepe = DataPack.data_dict_seperate_train_vali_test(\n",
    "        dataset_dict,\n",
    "        train_ratio=DataKeys['train_ratio'],\n",
    "        vali_ratio=DataKeys['valid_ratio'],\n",
    "        if_seed=(DataKeys['fix_random']>0),\n",
    "        seed=DataKeys['fix_random'] if DataKeys['fix_random']>0 else None\n",
    "    )\n",
    "    print (\"seperate raw dataset:\\n\")\n",
    "    print (dataset_dict_sepe)\n",
    "    # 3. do statistics on the dataset\n",
    "    num_key_list = [\n",
    "        # composition\n",
    "        'AlMolePC','NiMolePC','ErMolePC','ZrMolePC','YMolePC','YbMolePC',\\\n",
    "        # structure\n",
    "        'L12MolePC','ScheilL12MeltMolePC','ScheilL12MolePC',\\\n",
    "        'ScheilTerneryMeltMolePC','ScheilTernaryMolePC',\\\n",
    "        'ScheilAl3NiMeltMolePC','ScheilAl3NiMolePC',\\\n",
    "        'ScheilAl3ZrMeltMolePC','ScheilAl3ZrMolePC',\\\n",
    "        # properties\n",
    "        'BulkResistivity','Misfit','CoarseningRate',\\\n",
    "        'ScheilFRCutoff','ScheilFRMatrix','ScheilCSC','ScheilHCS'\n",
    "    ]\n",
    "    # draw_distri_of_dataset_dict(\n",
    "    #     dataset_dict_sepe,\n",
    "    #     num_key_list=num_key_list,\n",
    "    #     save_path=DataKeys['data_dir'],\n",
    "    #     if_slient_run=CKeys['if_slient_run'],\n",
    "    # )\n",
    "\n",
    "    # 4. create the tokenizer\n",
    "    tokenizer = DataPack.build_tokenizer(\n",
    "        tokenizer_type=DataKeys['tokenizer_type'],\n",
    "        tokenizer_file=DataKeys['tokenizer_file'],\n",
    "        seq_len=DataKeys['context_length'],\n",
    "    )\n",
    "\n",
    "    # 5. convert raw data into text data for GPT\n",
    "    # a. number record ==> sentences\n",
    "    sentence_dataset_dict_sepe = dataset_dict_sepe.map(\n",
    "        # self-defined function\n",
    "        DataPack.assemble_multi_sentence,\n",
    "        # other keys besides element in the procssing fun\n",
    "        fn_kwargs={\n",
    "            \"key_list\": ['Pred001', 'Gene001'],\n",
    "        },\n",
    "        batched= False, # True, # Note, sentences don't work for batched\n",
    "        # only keep those for training by removing the old ones\n",
    "        remove_columns=dataset_dict_sepe[\"train\"].column_names,\n",
    "    )\n",
    "    # b. sentences ==> tokenized sentences\n",
    "    tokenized_dataset_dict_sepe = sentence_dataset_dict_sepe.map(\n",
    "        # self-defined function\n",
    "        DataPack.tokenize_multi_sentence_with_BPE,\n",
    "        # other keys\n",
    "        fn_kwargs={\n",
    "            \"key_list\": ['Pred001', 'Gene001'],\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"context_length\": DataKeys['context_length'],\n",
    "        },\n",
    "        #\n",
    "        batched = True,\n",
    "        # only keep those for training by removing the old ones\n",
    "        remove_columns=sentence_dataset_dict_sepe[\"train\"].column_names,\n",
    "    )\n",
    "    # c. convert into dataloader\n",
    "    tokenized_dataset_dict_sepe.set_format(type=\"torch\")\n",
    "    # build into pytorch object\n",
    "    #\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_dataset_dict_sepe[\"train\"],\n",
    "        batch_size=DataKeys['batch_size'], # 32,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    #\n",
    "    eval_dataloader = DataLoader(\n",
    "        tokenized_dataset_dict_sepe[\"valid\"],\n",
    "        batch_size=DataKeys['batch_size'], # 32,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    # use for test\n",
    "    #\n",
    "    test_dataloader = DataLoader(\n",
    "        tokenized_dataset_dict_sepe[\"test\"],\n",
    "        batch_size=DataKeys['batch_size'], # 32,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    # store the key and results\n",
    "else:\n",
    "    # load back DataKeys\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa659495-432d-4e91-9143-04c12e17cad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "save the dataset and tokenizer....\n",
      "==========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311a2edc0aba416d9f673f7a386f2a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/471239 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14e4c33ebd545d19e71b8a606155977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/523 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e629767af6140b2b2de15e7cfd30455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/51837 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# save or recall necessary parts\n",
    "#\n",
    "if CKeys['Working_Mode']==1 and CKeys['IF_FirstRun']==1:\n",
    "    print (\"==========================================\")\n",
    "    print (\"save the dataset and tokenizer....\")\n",
    "    print (\"==========================================\")\n",
    "\n",
    "    # 1. on DataKeys\n",
    "    with open(PKeys['pk_data_pack'], 'wb') as handle:\n",
    "        pickle.dump(DataKeys, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    #\n",
    "    # 2. on tokenizer\n",
    "    # # --\n",
    "    # tokenizer.save_pretrained(\n",
    "    #     DataKeys['tokenizer_dir']\n",
    "    # )\n",
    "    # ++\n",
    "    DataPack.save_tokenizer(\n",
    "        DataKeys,\n",
    "        tokenizer,\n",
    "    )\n",
    "    #\n",
    "    # 3. dataloaders\n",
    "    # for trainining\n",
    "    torch.save(train_dataloader, DataKeys['data_dir']+'/train_dataloader.pt')\n",
    "    torch.save(eval_dataloader, DataKeys['data_dir']+'/eval_dataloader.pt')\n",
    "    torch.save(test_dataloader, DataKeys['data_dir']+'/test_dataloader.pt')\n",
    "\n",
    "    # 4. something else: sentence_data_dict\n",
    "    # hf_object\n",
    "    sentence_dataset_dict_sepe.save_to_disk(\n",
    "        DataKeys['data_dir']+'/sentence_dataset_dict_hf'\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"==========================================\")\n",
    "    print ('This is not a data-prepare run')\n",
    "    print ('Load back in the data packages...')\n",
    "    print(\"==========================================\")\n",
    "\n",
    "\n",
    "\n",
    "    with open(PKeys['pk_data_pack'], 'rb') as handle:\n",
    "        # data_pack = pickle.load(handle)\n",
    "        DataKeys = pickle.load(handle)\n",
    "\n",
    "\n",
    "    # on tokenizer\n",
    "    # # --\n",
    "    # from transformers import AutoTokenizer\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\n",
    "    #     DataKeys['tokenizer_dir']\n",
    "    # )\n",
    "    # ++\n",
    "    tokenizer  = DataPack.reload_tokenizer(\n",
    "        DataKeys\n",
    "    )\n",
    "\n",
    "    # on dataloaders\n",
    "    train_dataloader = torch.load(DataKeys['data_dir']+'/train_dataloader.pt')\n",
    "    eval_dataloader = torch.load(DataKeys['data_dir']+'/eval_dataloader.pt')\n",
    "    test_dataloader = torch.load(DataKeys['data_dir']+'/test_dataloader.pt')\n",
    "\n",
    "    # something else:\n",
    "    sentence_dataset_dict_sepe = load_from_disk(\n",
    "        DataKeys['data_dir']+'/sentence_dataset_dict_hf'\n",
    "    )\n",
    "\n",
    "    print ('Done.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba64acf2-7e07-4424-8a42-573295ca3282",
   "metadata": {},
   "source": [
    "## 4. On model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "921b3ab7-66aa-4966-9b60-63f9963ea931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Initialize the model key\n",
      "======================================\n",
      "Get the following keys...\n",
      "model_dir: \n",
      "./training_resu//1_model/\n",
      "model_type: \n",
      "6\n",
      "model_args: \n",
      "{'vocab_size': 256, 'n_layer': 36, 'block_size': 1024, 'n_embd': 1024, 'n_head': 16, 'bias': False, 'dropout': 0.2}\n",
      "Creating the given path...\n",
      "Done.\n",
      "==========================================\n",
      "Save the model config ...\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "if CKeys['Working_Mode']==1 and CKeys['IF_FirstRun']==1:\n",
    "\n",
    "    print (\"======================================\")\n",
    "    print (\"Initialize the model key\")\n",
    "    print (\"======================================\")\n",
    "    ModelKeys = {}\n",
    "    # pass on the keys\n",
    "    print (\"Get the following keys...\")\n",
    "    for this_key in PKeys['Model_PKeys'].keys():\n",
    "        print (\"{}: \\n{}\".format(this_key, PKeys['Model_PKeys'][this_key]))\n",
    "        ModelKeys[this_key] = PKeys['Model_PKeys'][this_key]\n",
    "\n",
    "    # 1. create the folder\n",
    "    UtilityPack.create_path(ModelKeys['model_dir'])\n",
    "    # 2. deliver one key for model building\n",
    "    model_args = ModelKeys['model_args']\n",
    "\n",
    "\n",
    "    print(\"==========================================\")\n",
    "    print(\"Save the model config ...\")\n",
    "    print(\"==========================================\")\n",
    "    # on ModelKeys\n",
    "    with open(PKeys['pk_model_pack'], 'wb') as handle:\n",
    "        pickle.dump(ModelKeys, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "else:\n",
    "    # this can be: 2nd training run or test run\n",
    "    # load in the keys\n",
    "\n",
    "    print(\"==========================================\")\n",
    "    print ('This is not the first run')\n",
    "    print ('Load back in the model packages...')\n",
    "    print(\"==========================================\")\n",
    "\n",
    "    with open(PKeys['pk_model_pack'], 'rb') as handle:\n",
    "        # data_pack = pickle.load(handle)\n",
    "        ModelKeys = pickle.load(handle)\n",
    "\n",
    "    # unpack the model_args key\n",
    "    model_args = ModelKeys['model_args']\n",
    "    print (\"model_args: \\n\", model_args)\n",
    "\n",
    "    # From Trainer to know what parameters to load in\n",
    "    with open(PKeys['pk_train_pack'], 'rb') as handle:\n",
    "        # data_pack = pickle.load(handle)\n",
    "        TrainKeys = pickle.load(handle)\n",
    "    # # unpack the model_args key\n",
    "    # model_args = ModelKeys['model_args']\n",
    "    print (\"TrainKeys: \\n\", TrainKeys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70bfaa9b-3a82-4c6c-ab8f-1edf9c22ad8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model dir: ./training_resu//1_model/\n"
     ]
    }
   ],
   "source": [
    "print (\n",
    "    f\"model dir: \"+\n",
    "    ModelKeys['model_dir']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c181bc5e-0e6e-48a3-96d9-91b7572e04e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# needed for the second run\n",
    "# ====================================================\n",
    "# load in the model if this is not first train\n",
    "#\n",
    "if CKeys['Working_Mode']>0 and CKeys['IF_FirstRun']>1:\n",
    "    #\n",
    "    if CKeys['Resume_from_where'] == 'LAST':\n",
    "        ckpt_name = TrainKeys['out_dir_last']+'Last_ckpt.pt'\n",
    "    elif CKeys['Resume_from_where'] == 'BEST':\n",
    "        ckpt_name = TrainKeys['out_dir_best']+'Best_ckpt.pt'\n",
    "    #\n",
    "    print (\"CK_PT: \", ckpt_name)\n",
    "    #\n",
    "    # prepare to resume training from a checkpoint\n",
    "    checkpoint = torch.load(ckpt_name, map_location=TrainKeys['device'])\n",
    "    checkpoint_model_args = checkpoint['model_args']\n",
    "    #\n",
    "    # force these config attributes to be equal otherwise we can't even resume training\n",
    "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
    "    #\n",
    "    # For new models, this is just a check\n",
    "    # |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
    "    # problem-specified\n",
    "    for k in checkpoint_model_args.keys():\n",
    "        if model_args[k] != checkpoint_model_args[k]:\n",
    "            print (\"hard update needed on \", k)\n",
    "            print (\"old: \", model_args[k])\n",
    "            print (\"new: \", checkpoint_model_args[k])\n",
    "            model_args[k] = checkpoint_model_args[k]\n",
    "    print (\"Updated model_args: \", model_args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ad82ccd-a9c2-4c11-847d-62e8535d8197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "Initialize the model from the scratch...\n",
      "=========================================\n",
      "number of parameters: 453.32M\n",
      "\n",
      "\n",
      "\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(256, 1024)\n",
      "    (wpe): Embedding(1024, 1024)\n",
      "    (drop): Dropout(p=0.2, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-35): 36 x MHA_Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "if CKeys['Working_Mode']>0:\n",
    "    print (\"=========================================\")\n",
    "    print (\"Initialize the model from the scratch...\")\n",
    "    print (\"=========================================\")\n",
    "    #\n",
    "    model_config = ModelPack.build_model_config(\n",
    "        CKeys,\n",
    "        model_args\n",
    "    )\n",
    "    #\n",
    "    model = ModelPack.build_model(\n",
    "        CKeys,\n",
    "        model_config,\n",
    "    )\n",
    "\n",
    "    # # may add another IF to distinguish the model types\n",
    "    # #\n",
    "    # # process: model configuration\n",
    "    # gptconf = GPTConfig(**model_args)\n",
    "    # print (\"Recieve model config: \\n\", gptconf)\n",
    "    # # process: model\n",
    "    # model = GPT(gptconf)\n",
    "\n",
    "    print (\"\\n\\n\")\n",
    "    print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bb563f1-dbe8-45f6-a32c-0f07420db517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "Load back the model if this's not the 1st training.\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "print (\"===================================================\")\n",
    "print (\"Load back the model if this's not the 1st training.\")\n",
    "print (\"===================================================\")\n",
    "#\n",
    "if CKeys['Working_Mode']>0 and CKeys['IF_FirstRun']>1:\n",
    "    # Here, for GPT, we use checkpoint to load back the model\n",
    "    print (\"Loading the saved model...\")\n",
    "    #\n",
    "    state_dict = checkpoint['model']\n",
    "    # NOTE, this one has unwanted contents.\n",
    "    # fix the keys of the state dictionary :(\n",
    "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    #\n",
    "    # load back the previous breaking point\n",
    "    print (\"Load in the model...\")\n",
    "    model.load_state_dict(state_dict)\n",
    "    print (\"Update some other recrods...\")\n",
    "    # clean\n",
    "    state_dict = None\n",
    "    print (\"Done\")\n",
    "    # TBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1372553-c2f4-4762-a4b6-8b61d6e057ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================\n",
      "Consider loading back the trained model IF available...\n",
      "===========================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# consider loading back the trained model\n",
    "print ('===========================================================')\n",
    "print ('Consider loading back the trained model IF available...')\n",
    "print ('===========================================================')\n",
    "#\n",
    "if CKeys['Working_Mode']>0 and CKeys['IF_FirstRun']>1:\n",
    "    # Here, for GPT, we use checkpoint to load back the model\n",
    "    print (\"Loading the training history...\")\n",
    "    # ref-code:\n",
    "    # checkpoint = {\n",
    "    #                 'model': raw_model.state_dict(),\n",
    "    #                 'optimizer': optimizer.state_dict(),\n",
    "    #                 'model_args': model_args,\n",
    "    #                 'completed_updating_steps': completed_updating_steps,\n",
    "    #                 'step_num': this_step,\n",
    "    #                 'iter_num_at_best_loss': GAS_at_best_val_loss,\n",
    "    #                 'best_val_loss': best_val_loss\n",
    "    #             }\n",
    "    best_val_loss_0 = checkpoint['best_val_loss']\n",
    "    GAS_at_best_val_loss_0 = checkpoint['iter_num_at_best_loss'] # -100\n",
    "    # finished_steps_0 = checkpoint['iter_num']*TrainKeys['gradient_accumulation_steps']\n",
    "    finished_steps_0 = checkpoint['step_num']\n",
    "    completed_updating_steps_0 = checkpoint['completed_updating_steps']\n",
    "    #\n",
    "    print (\"On the read checkpoint:\")\n",
    "    print (f\"step_num: {finished_steps_0}\")\n",
    "    print (f\"GAS_num: {completed_updating_steps_0}\")\n",
    "    print (f\"best_val_loss: {best_val_loss_0}\")\n",
    "    print (f\"GAS_at_best_val_loss: {GAS_at_best_val_loss_0}\")\n",
    "    # TBA\n",
    "    # # cleaning\n",
    "    # checkpoint=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928c2167-e92d-4d31-9c00-8a9dcf430800",
   "metadata": {},
   "source": [
    "## 5. On training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3c7dd34-e823-4c7c-a92b-e65607fd71fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a59276f-7db3-4923-afe2-73078be74c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================\n",
      "Initialize training at 1st run ...\n",
      "===========================================================\n",
      "Get the following keys...\n",
      "out_dir: \n",
      "./training_resu//1_model//training_dir/\n",
      "batch_size: \n",
      "4\n",
      "block_size: \n",
      "1024\n",
      "vocab_size: \n",
      "256\n",
      "num_train_epochs: \n",
      "5\n",
      "gradient_accumulation_steps: \n",
      "8\n",
      "weight_decay: \n",
      "0.1\n",
      "beta1: \n",
      "0.9\n",
      "beta2: \n",
      "0.95\n",
      "grad_clip: \n",
      "1.0\n",
      "warmup_iters: \n",
      "1000\n",
      "learning_rate: \n",
      "0.0006\n",
      "decay_lr: \n",
      "True\n",
      "min_lr: \n",
      "5.9999999999999995e-05\n",
      "report_1_trai_loss_this_GAS: \n",
      "20\n",
      "report_2_vali_pred_this_GAS: \n",
      "200\n",
      "report_3_save_mode_this_GAS: \n",
      "400\n",
      "backend: \n",
      "nccl\n",
      "device: \n",
      "cuda\n",
      "dtype: \n",
      "bfloat16\n",
      "compile: \n",
      "True\n",
      "Creating the given path...\n",
      "Done.\n",
      "Creating the given path...\n",
      "Done.\n",
      "Creating the given path...\n",
      "Done.\n",
      "==========================================\n",
      "Save the train config ...\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "# Training mode\n",
    "if CKeys['Working_Mode']==1 and CKeys['IF_FirstRun']==1:\n",
    "\n",
    "    print ('===========================================================')\n",
    "    print ('Initialize training at 1st run ...')\n",
    "    print ('===========================================================')\n",
    "\n",
    "    TrainKeys = {}\n",
    "    # pass on the keys\n",
    "    print (\"Get the following keys...\")\n",
    "    for this_key in PKeys['Train_PKeys'].keys():\n",
    "        print (\"{}: \\n{}\".format(this_key, PKeys['Train_PKeys'][this_key]))\n",
    "        TrainKeys[this_key] = PKeys['Train_PKeys'][this_key]\n",
    "\n",
    "    # 1. create dir\n",
    "    UtilityPack.create_path(TrainKeys['out_dir'])\n",
    "    # two subdir just for model checkpoint\n",
    "    TrainKeys['out_dir_last'] = TrainKeys['out_dir'] + \"last_check/\"\n",
    "    TrainKeys['out_dir_best'] = TrainKeys['out_dir'] + \"best_check/\"\n",
    "    UtilityPack.create_path(TrainKeys['out_dir_last'])\n",
    "    UtilityPack.create_path(TrainKeys['out_dir_best'])\n",
    "    #\n",
    "    # Some that can only be defined here\n",
    "    TrainKeys['max_iters'] = math.ceil(\n",
    "        len(train_dataloader)/TrainKeys['gradient_accumulation_steps']\n",
    "    ) * TrainKeys['num_train_epochs']  # in GAS\n",
    "    TrainKeys['lr_decay_iters'] = TrainKeys['max_iters']\n",
    "\n",
    "    # 2. process: problem-specific ones\n",
    "    # ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
    "    # initialize the recrods\n",
    "    best_val_loss_0 = 1.E09\n",
    "    GAS_at_best_val_loss_0 = -100\n",
    "    finished_steps_0 = 0\n",
    "    completed_updating_steps_0 = 0\n",
    "\n",
    "    # TrainKeys['best_val_loss'] = 1.e09\n",
    "    # TrainKeys['GAS_at_best_val_loss'] = -100\n",
    "\n",
    "    # =========================================================\n",
    "    # below are not from PKeys\n",
    "    # secondary ones\n",
    "    # files: use fixed ones\n",
    "    TrainKeys['1_train_loss.log']=TrainKeys['out_dir']+'1_train_loss.log'\n",
    "    TrainKeys['2_vali_loss.log']=TrainKeys['out_dir']+'2_vali_loss.log'\n",
    "    TrainKeys['2_vali_gene.log']=TrainKeys['out_dir']+'2_vali_gene.log'\n",
    "    # # --\n",
    "    # TrainKeys['3_save_model.log']=TrainKeys['out_dir']+'3_save_model.log'\n",
    "    # ++\n",
    "    TrainKeys['3_save_model_last.log']=TrainKeys['out_dir_last']+'3_save_model_last.log'\n",
    "    TrainKeys['3_save_model_best.log']=TrainKeys['out_dir_best']+'3_save_model_best.log'\n",
    "\n",
    "    # 3. save the TrainKeys\n",
    "    print(\"==========================================\")\n",
    "    print(\"Save the train config ...\")\n",
    "    print(\"==========================================\")\n",
    "\n",
    "    # on ModelKeys\n",
    "    with open(PKeys['pk_train_pack'], 'wb') as handle:\n",
    "        pickle.dump(TrainKeys, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "elif CKeys['Working_Mode']==1 and CKeys['IF_FirstRun']>1:\n",
    "\n",
    "    print(\"==========================================\")\n",
    "    print ('This is not the first run')\n",
    "    print ('Load back in the train packages...')\n",
    "    print ('Done in the previous block with ModelKeys')\n",
    "    print(\"==========================================\")\n",
    "\n",
    "else:\n",
    "    # this a test mode: do whatever needed\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b7f3bff-ac9c-4f6c-8406-3ca4440d7d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the status before the preparation...\n",
      "allow_ft32 in cuda: \n",
      " False\n",
      "allow_ft32 in cudnn: \n",
      " True\n"
     ]
    }
   ],
   "source": [
    "if CKeys['Working_Mode']==1:\n",
    "    print (\"Check the status before the preparation...\")\n",
    "    print (\"allow_ft32 in cuda: \\n\",\n",
    "           torch.backends.cuda.matmul.allow_tf32)\n",
    "    print (\"allow_ft32 in cudnn: \\n\",\n",
    "           torch.backends.cudnn.allow_tf32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7623ac5-ded0-4d98-b71c-297b5d2db2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if CKeys['Working_Mode']==1 and CKeys['Debug_Train']==1:\n",
    "    print ({**TrainKeys, **ModelKeys})\n",
    "    print (TrainKeys['dtype'])\n",
    "    print (TrainKeys['compile'])\n",
    "    # print (TrainKeys['ddp']) # this one comes only after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2b95b51-eab6-4639-9277-b45287b836ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old config: \n",
      " {'out_dir': './training_resu//1_model//training_dir/', 'batch_size': 4, 'block_size': 1024, 'vocab_size': 256, 'num_train_epochs': 5, 'gradient_accumulation_steps': 8, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'warmup_iters': 1000, 'learning_rate': 0.0006, 'decay_lr': True, 'min_lr': 5.9999999999999995e-05, 'report_1_trai_loss_this_GAS': 20, 'report_2_vali_pred_this_GAS': 200, 'report_3_save_mode_this_GAS': 400, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'bfloat16', 'compile': True, 'out_dir_last': './training_resu//1_model//training_dir/last_check/', 'out_dir_best': './training_resu//1_model//training_dir/best_check/', 'max_iters': 147265, 'lr_decay_iters': 147265, '1_train_loss.log': './training_resu//1_model//training_dir/1_train_loss.log', '2_vali_loss.log': './training_resu//1_model//training_dir/2_vali_loss.log', '2_vali_gene.log': './training_resu//1_model//training_dir/2_vali_gene.log', '3_save_model_last.log': './training_resu//1_model//training_dir/last_check/3_save_model_last.log', '3_save_model_best.log': './training_resu//1_model//training_dir/best_check/3_save_model_best.log', 'model_dir': './training_resu//1_model/', 'model_type': 6, 'model_args': {'vocab_size': 256, 'n_layer': 36, 'block_size': 1024, 'n_embd': 1024, 'n_head': 16, 'bias': False, 'dropout': 0.2}}\n",
      "tokens per iteration/GAS will be: 32,768\n",
      "Is this a DDP run:  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_1098514/513670383.py:31: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(TrainKeys['dtype'] == 'float16'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 146, with 454,295,552 parameters\n",
      "num non-decayed parameter tensors: 73, with 74,752 parameters\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trace/group/tmousavi/bni2/1_JupyterGit/workspace/AlloyGPT_InternalTest_0/AlloyGPT_env/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new config: \n",
      " {'out_dir': './training_resu//1_model//training_dir/', 'batch_size': 4, 'block_size': 1024, 'vocab_size': 256, 'num_train_epochs': 5, 'gradient_accumulation_steps': 8, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'warmup_iters': 1000, 'learning_rate': 0.0006, 'decay_lr': True, 'min_lr': 5.9999999999999995e-05, 'report_1_trai_loss_this_GAS': 20, 'report_2_vali_pred_this_GAS': 200, 'report_3_save_mode_this_GAS': 400, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'bfloat16', 'compile': True, 'out_dir_last': './training_resu//1_model//training_dir/last_check/', 'out_dir_best': './training_resu//1_model//training_dir/best_check/', 'max_iters': 147265, 'lr_decay_iters': 147265, '1_train_loss.log': './training_resu//1_model//training_dir/1_train_loss.log', '2_vali_loss.log': './training_resu//1_model//training_dir/2_vali_loss.log', '2_vali_gene.log': './training_resu//1_model//training_dir/2_vali_gene.log', '3_save_model_last.log': './training_resu//1_model//training_dir/last_check/3_save_model_last.log', '3_save_model_best.log': './training_resu//1_model//training_dir/best_check/3_save_model_best.log', 'master_process': True, 'seed_offset': 0, 'ddp_world_size': 1, 'ddp_local_rank': None, 'ddp_rank': None, 'tokens_per_iter': 32768, 'ptdtype': torch.bfloat16, 'device_type': 'cuda', 'ddp': False, 'model_dir': './training_resu//1_model/', 'model_type': 6, 'model_args': {'vocab_size': 256, 'n_layer': 36, 'block_size': 1024, 'n_embd': 1024, 'n_head': 16, 'bias': False, 'dropout': 0.2}}\n"
     ]
    }
   ],
   "source": [
    "if CKeys['Working_Mode']==1:\n",
    "    # prepare for training\n",
    "    # ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
    "    # This block is problem-specified\n",
    "    #\n",
    "    # before ddp, make a record of model+trainer:\n",
    "    config = {**TrainKeys, **ModelKeys}\n",
    "    print (\"old config: \\n\", config)\n",
    "    #\n",
    "    # XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "    # ATTENSION: afte this step, ranks matter\n",
    "    # XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "    #\n",
    "    TrainKeys, ctx = TrainPack.initialize_train_fun(TrainKeys)\n",
    "    # this one updates the TrainKeys based on whether this is the master_process\n",
    "    # and create folder if it is master_process\n",
    "    #\n",
    "    #\n",
    "    print (\"Is this a DDP run: \", TrainKeys['ddp'])\n",
    "    #\n",
    "    # crop down the model block size if desired, using model surgery\n",
    "    if TrainKeys['block_size'] < model.config.block_size:\n",
    "        print (\"crop down the model block size\")\n",
    "        model.crop_block_size(TrainKeys['block_size'])\n",
    "        model_args['block_size'] = TrainKeys['block_size'] # so that the checkpoint will have the right value\n",
    "    #\n",
    "    # move the model to the device\n",
    "    model.to(TrainKeys['device'])\n",
    "    #\n",
    "    # initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(TrainKeys['dtype'] == 'float16'))\n",
    "    #\n",
    "    # optimizer: initilize\n",
    "    optimizer = model.configure_optimizers(\n",
    "        TrainKeys['weight_decay'],\n",
    "        TrainKeys['learning_rate'],\n",
    "        (TrainKeys['beta1'], TrainKeys['beta2']),\n",
    "        TrainKeys['device_type']\n",
    "    )\n",
    "    # load it back\n",
    "    if CKeys['IF_FirstRun'] !=1:\n",
    "        print (\"load in optimizer from a previous run\")\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    #\n",
    "    # initilize for 1st run\n",
    "    # free up memory if not 1st run;\n",
    "    checkpoint = None # free up memory\n",
    "    #\n",
    "    # compile the model\n",
    "    if TrainKeys['compile']:\n",
    "        print(\"compiling the model... (takes a ~minute)\")\n",
    "        # https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning\n",
    "        # import os\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\" # \"false\"\n",
    "\n",
    "        unoptimized_model = model\n",
    "        model = torch.compile(model) # requires PyTorch 2.0\n",
    "    # wrap model into DDP container\n",
    "    if TrainKeys['ddp']:\n",
    "        print (\"adjust model through DDP...\")\n",
    "        model = DDP(model, device_ids=[TrainKeys['ddp_local_rank']])\n",
    "    #\n",
    "    # loagging\n",
    "    config = {**TrainKeys, **ModelKeys}\n",
    "    print (\"new config: \\n\", config)\n",
    "    # if TrainKeys['wandb_log'] and TrainKeys['master_process']:\n",
    "    #     import wandb\n",
    "    #     wandb.init(\n",
    "    #         project=TrainKeys['wandb_project'],\n",
    "    #         name=TrainKeys['wandb_run_name'],\n",
    "    #         config=config,\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f07f2ee1-09ac-4468-8f3f-573575680199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the status After the preparation...\n",
      "allow_ft32 in cuda: \n",
      " True\n",
      "allow_ft32 in cudnn: \n",
      " True\n"
     ]
    }
   ],
   "source": [
    "if CKeys['Working_Mode']==1:\n",
    "    print (\"Check the status After the preparation...\")\n",
    "    print (\"allow_ft32 in cuda: \\n\",\n",
    "           torch.backends.cuda.matmul.allow_tf32)\n",
    "    print (\"allow_ft32 in cudnn: \\n\",\n",
    "           torch.backends.cudnn.allow_tf32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df0f3be3-67c9-40bf-9d55-5cc8e5967341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IF this is master_process:  True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if CKeys['Working_Mode']==1:\n",
    "    print (\"IF this is master_process: \", TrainKeys['master_process'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46a71666-424a-47b8-b83d-7eaceecf3378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "perpare for the 1st training run ...\n",
      "==============================================\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# prepare to record the training history\n",
    "#\n",
    "if CKeys['Working_Mode']==1 and CKeys['IF_FirstRun'] == 1:\n",
    "    print (\"==============================================\")\n",
    "    print (\"perpare for the 1st training run ...\")\n",
    "    print (\"==============================================\")\n",
    "    #\n",
    "    # ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
    "    # This is problem-specified\n",
    "    #\n",
    "    # for 1st run, initialize the logs\n",
    "    if TrainKeys['master_process']:\n",
    "        # 1. train loss\n",
    "        top_line = f\"epoch,step,GAS,wei_loss_trai,plain_loss_trai,lr\\n\"\n",
    "        UtilityPack.add_one_line_to_file(\n",
    "            file_name=TrainKeys['1_train_loss.log'],\n",
    "            this_line=top_line,\n",
    "            mode='w', # if exitst, erase it\n",
    "        )\n",
    "        # 2. eval loss\n",
    "        # f\"epoch: %d, step: %d, GAS: %d, wei_loss/trai: %f, plain_loss/trai: %f, loss/eval: %f, lr: %f\\n\"\n",
    "        top_line = f\"epoch,step,GAS,wei_loss_trai,plain_loss_trai,loss_eval,lr\\n\"\n",
    "        UtilityPack.add_one_line_to_file(\n",
    "            file_name=TrainKeys['2_vali_loss.log'],\n",
    "            this_line=top_line,\n",
    "            mode='w', # if exitst, erase it\n",
    "        )\n",
    "        #\n",
    "        # 3. predict lines\n",
    "        UtilityPack.add_one_line_to_file(\n",
    "            file_name=TrainKeys['2_vali_gene.log'],\n",
    "            this_line='\\n',\n",
    "            mode='w', # if exitst, erase it\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50c75a11-af51-44f9-a2eb-9a1e6c592b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "8\n",
      "cuda\n",
      "cuda\n",
      "1000\n",
      "0.0006\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if CKeys['Working_Mode']==1:\n",
    "    print (device)\n",
    "    # print (TrainKeys['ddp'])\n",
    "    # print (ddp)\n",
    "    print (TrainKeys['gradient_accumulation_steps'])\n",
    "    print (TrainKeys['device_type'])\n",
    "    print (TrainKeys['device'])\n",
    "    print (TrainKeys['warmup_iters'])\n",
    "    print (TrainKeys['learning_rate'])\n",
    "    print (TrainKeys['decay_lr'])\n",
    "    print (TrainKeys['master_process'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c30ffe4b-e4df-471c-ab1e-38d202ae01e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if CKeys['Working_Mode']==1:\n",
    "    # input\n",
    "    # test prompts for training\n",
    "    test_prompts_during_train = [\n",
    "        \"{Task:Pred001}={Composition:[Al:+9.440e+01,Ni:+3.552e+00,Er:+4.962e-01,Zr:+3.085e-01,Y:+7.322e-01,Yb:+5.108e-01]}=>{Structure:\",\n",
    "        \"{Task:Gene001}={Property:[BulkResistivity:+4.514e-01,Misfit:+9.770e-01,CoarseningRate:+2.164e+00,ScheilFRCutoff:+1.412e+02,ScheilFRMatrix:+1.699e+01,ScheilCSC:+2.172e-01,ScheilHCS:+3.067e-01]}=>{Structure\",\n",
    "    ]\n",
    "    # update for remaked one\n",
    "    test_prompts_during_train = [\n",
    "        \"{Task:Pred001}={Composition:[(Al):+9.388e+01,(Ni):+3.656e+00,(Er):+1.439e+00,(Zr):+3.195e-01,(Y):+4.847e-01,(Yb):+2.209e-01]}=>{Structure:\",\n",
    "        \"{Task:Gene001}={Property:[DiffusionResistivity:+1.696e-01,Misfit:+1.257e+00,CoarseningMetric:+4.000e+00,FreezingRange:+1.318e-01,CrackSusceptibilityCoefficient:+0.000e+00,HotCrackingSusceptibility:+0.000e+00]}=>{Structure:\"\n",
    "    ]\n",
    "    # key tokens\n",
    "    target_keywords = [\n",
    "        \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\n",
    "        \".\",\"+\",\"-\",\"e\",\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f1de568-88d1-4c37-ab8f-3cc5851f0dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[129, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 158]\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# prepare training\n",
    "#\n",
    "if CKeys['Working_Mode']==1:\n",
    "    #\n",
    "    # 1. key tokens\n",
    "    keytoken_ids = TrainPack.translate_words_w_tokenizers(\n",
    "        tokenizer,\n",
    "        target_keywords,\n",
    "    )\n",
    "    print (keytoken_ids)\n",
    "    #\n",
    "    # 2. translate that into weight\n",
    "    wei_list_for_all_vocab = TrainPack.build_weight_list_for_vocab(\n",
    "        model.config.vocab_size,\n",
    "        keytoken_ids,\n",
    "        wei_0=1.,\n",
    "        wei_1=2.,\n",
    "    ).to(device)\n",
    "    print (wei_list_for_all_vocab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243fbee6-7fcf-494e-953d-b707eb383a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start the training loop...\n",
      "####################\n",
      "\n",
      "epoch: 0, step: 160, GAS: 20, wei_loss/trai: 1.268766, plain_loss/trai: 1.152305, lr: 0.000012\n",
      "\n",
      "####################\n",
      "\n",
      "epoch: 0, step: 320, GAS: 40, wei_loss/trai: 0.671931, plain_loss/trai: 0.545766, lr: 0.000024\n",
      "\n",
      "####################\n",
      "\n",
      "epoch: 0, step: 480, GAS: 60, wei_loss/trai: 0.301311, plain_loss/trai: 0.185495, lr: 0.000036\n",
      "\n",
      "####################\n",
      "\n",
      "epoch: 0, step: 640, GAS: 80, wei_loss/trai: 0.315458, plain_loss/trai: 0.194371, lr: 0.000048\n",
      "\n",
      "####################\n",
      "\n",
      "epoch: 0, step: 800, GAS: 100, wei_loss/trai: 0.378221, plain_loss/trai: 0.248343, lr: 0.000060\n",
      "\n",
      "####################\n",
      "\n",
      "epoch: 0, step: 960, GAS: 120, wei_loss/trai: 0.295191, plain_loss/trai: 0.180656, lr: 0.000072\n",
      "\n",
      "####################\n",
      "\n",
      "epoch: 0, step: 1120, GAS: 140, wei_loss/trai: 0.295083, plain_loss/trai: 0.181643, lr: 0.000084\n",
      "\n",
      "####################\n",
      "\n",
      "epoch: 0, step: 1280, GAS: 160, wei_loss/trai: 0.390396, plain_loss/trai: 0.258548, lr: 0.000096\n",
      "\n",
      "####################\n",
      "\n",
      "epoch: 0, step: 1440, GAS: 180, wei_loss/trai: 0.274075, plain_loss/trai: 0.167972, lr: 0.000108\n",
      "\n",
      "####################\n",
      "\n",
      "epoch: 0, step: 1600, GAS: 200, wei_loss/trai: 0.306828, plain_loss/trai: 0.189326, lr: 0.000120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 20/262 [00:17<03:37,  1.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 1600, GAS: 200, wei_loss/trai: 0.306828, plain_loss/trai: 0.189326, loss/eval: 0.236367, lr: 0.000120\n",
      "\n",
      "0: \n",
      "{Task:Pred001}={Composition:[(Al):+9.388e+01,(Ni):+3.656e+00,(Er):+1.439e+00,(Zr):+3.195e-01,(Y):+4.847e-01,(Yb):+2.209e-01]}=>{Structure:[AsBuilt_L12Mol%:+8.258e+01,AsBuilt_TernaryMol%:+6.593e+01,AsBuilt_Al3NiMol%:+0.371e+01,AsBuilt_Al3ZrMol%:+5.306e+00,L12Mol%:+1.782e+01,TernaryMol%:+0.398e+00,Al3NiMol%:+3.658e+00,Al3ZrMol%:+8.900e+00]}=>{Property:[DiffusionResistivity:+1.083e-01,Misfit:+1.260e+00,CoarseningMetric:+1.132e+00,FreezingRange:+1.307e-00,CrackSusceptibilityCoefficient:+2.615e-01,HotCrackingSusceptibility:+1.496e-01]}\n",
      "\n",
      "1: \n",
      "{Task:Gene001}={Property:[DiffusionResistivity:+1.696e-01,Misfit:+1.257e+00,CoarseningMetric:+4.000e+00,FreezingRange:+1.318e-01,CrackSusceptibilityCoefficient:+0.000e+00,HotCrackingSusceptibility:+0.000e+00]}=>{Structure:[AsBuilt_L1e+00,L12Mol%:+01,AsBuilt_TernaryMol%:+4.76e+00,Al3NiMol%:+1.000e+01,Al3ZrMol%:+0.621e+00]}=>{Property:[DiffusionResistivity:+9.254e-00,Misfit:+1.176e-01,CoarseningMetric:+1.864e+01,FreezingRange:+1.438e+00,CrackSusceptibilityCoefficient:+2.436e-01,HotCrackingSusceptibility:+1.428e-01]}\n",
      "\n",
      "####################\n",
      "\n",
      "epoch: 0, step: 1760, GAS: 220, wei_loss/trai: 0.468329, plain_loss/trai: 0.317157, lr: 0.000132\n",
      "\n",
      "####################\n",
      "\n",
      "epoch: 0, step: 1920, GAS: 240, wei_loss/trai: 0.273547, plain_loss/trai: 0.167698, lr: 0.000144\n",
      "\n",
      "####################\n",
      "\n",
      "epoch: 0, step: 2080, GAS: 260, wei_loss/trai: 0.271771, plain_loss/trai: 0.167972, lr: 0.000156\n",
      "\n",
      "####################\n",
      "\n",
      "epoch: 0, step: 2240, GAS: 280, wei_loss/trai: 0.270822, plain_loss/trai: 0.165457, lr: 0.000168\n",
      "\n",
      "####################\n",
      "\n",
      "epoch: 0, step: 2400, GAS: 300, wei_loss/trai: 0.257654, plain_loss/trai: 0.157752, lr: 0.000180\n",
      "\n",
      "##################"
     ]
    }
   ],
   "source": [
    "if CKeys['Working_Mode'] == 1:\n",
    "    # for some \n",
    "    import torch._dynamo\n",
    "    torch._dynamo.config.suppress_errors = True\n",
    "    \n",
    "    print (\"Start the training loop...\")\n",
    "    eval_size = 20\n",
    "\n",
    "    TrainPack.training_loop_AR_LM(\n",
    "        #\n",
    "        model,\n",
    "        tokenizer,\n",
    "        ctx,\n",
    "        optimizer,\n",
    "        TrainKeys,\n",
    "        train_dataloader,\n",
    "        eval_dataloader,\n",
    "        eval_size,\n",
    "        wei_list_for_all_vocab,\n",
    "        scaler,\n",
    "        test_prompts_during_train,\n",
    "        #\n",
    "        best_val_loss_0,\n",
    "        GAS_at_best_val_loss_0,\n",
    "        finished_steps_0,\n",
    "        completed_updating_steps_0,\n",
    "        model_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4510b-4d0c-41b3-8ee9-5eb89879d7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AlloyGPT_env)",
   "language": "python",
   "name": "alloygpt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
